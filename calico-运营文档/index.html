<!doctype html><html lang=en><head><script async defer data-website-id=08a754c4-6690-46d7-bb80-8ff93cfa232f src=https://umami.oldwinter.top/umami.js></script><meta charset=utf-8><meta name=description content="新增 or 扩容 ippool 固定 ip ns 与 subnet 绑定 calico 限速 calico 的 tunnel 地址会自动分配 迁移 ippool 的容量准备 blockSize 应该如何取值  kubesphere 集成了cni 的 ippool 和 网络策略，ks 在 cni 基础之上封装了一层，用于屏蔽不同 cni 底层实现的，现阶段版本中底层中主要使用了 calico。"><title>calico 运营文档</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://www.ryken.cloud//icon.png><link href=https://www.ryken.cloud/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://www.ryken.cloud/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://www.ryken.cloud/js/darkmode.66c3ed4615b59e04fa897375e67b0fe2.min.js></script>
<script src=https://www.ryken.cloud/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://www.ryken.cloud/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://www.ryken.cloud/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://www.ryken.cloud/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://www.ryken.cloud/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://www.ryken.cloud/",fetchData=Promise.all([fetch("https://www.ryken.cloud/indices/linkIndex.b01fa6323962ab5e01c52f78b671cbef.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://www.ryken.cloud/indices/contentIndex.1b8dc94e6cc57cccaf3b495f2b603c39.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addCollapsibleCallouts(),initPopover("https://www.ryken.cloud",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://www.ryken.cloud",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},window.addEventListener("DOMContentLoaded",()=>{init(),render()})</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-11MD77L81V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-11MD77L81V",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=search placeholder=支持标题及全文搜索...><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://www.ryken.cloud/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://www.ryken.cloud/>Knowledge Garden</a></h1><div class=spacer></div><div id=search-icon><p>search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>calico 运营文档</h1><p class=meta>最近编辑于
Apr 15, 2024</p><ul class=tags></ul><aside class=mainTOC><details><summary>目录</summary><nav id=TableOfContents><ol><li><ol><li><a href=#1-ippool-新增-or-扩容>1. ippool 新增 or 扩容</a></li><li><a href=#2-固定-ip>2. 固定 ip</a></li></ol></li></ol></nav></details></aside><ol><li>新增 or 扩容 ippool</li><li>固定 ip</li><li>ns 与 subnet 绑定</li><li>calico 限速</li><li>calico 的 tunnel 地址会自动分配</li><li>迁移 ippool 的容量准备</li><li>blockSize 应该如何取值</li></ol><p>kubesphere 集成了cni 的 ippool 和 网络策略，ks 在 cni 基础之上封装了一层，用于屏蔽不同 cni 底层实现的，现阶段版本中底层中主要使用了 calico。</p><a href=#1-ippool-新增-or-扩容><h3 id=1-ippool-新增-or-扩容><span class=hanchor arialabel=Anchor># </span>1. ippool 新增 or 扩容</h3></a><p>扩容Calico的IPV4POOL时，会对业务以及网络有影响吗？需要做哪些配合工作？</p><a href=#新增-ippool><h4 id=新增-ippool><span class=hanchor arialabel=Anchor># </span>新增 ippool</h4></a><p>使用 kk 安装集群时会使用 kubePodsCIDR: 10.233.64.0/18 创建一个默认子网 default-ipv4-ippool，此时创建的工作负载会使用该 ippool 来分配 ip，当然也可以新建新的 ippool 来用于负载的 ip 分配，如果某一个 ippool 因地址分配满而不够使用时，可以再创建一个新的 ippool 来分配 ip。
在 overlay 网络模式下，可以无需关系网络细节，但是在 underlay 模式下，使用 bgp 模式时新建的 ippool 需要注意不要与已有的 ip 地址段冲突，并且根据实际情况考虑是否需要在对应的交换机上添加对应的策略来放行或者拒绝新的 ippool 的地址段。</p><a href=#扩容-ippool><h4 id=扩容-ippool><span class=hanchor arialabel=Anchor># </span>扩容 ippool</h4></a><p>如果可以通过新建 ippool 的方式解决 ip 地址不够用的问题，则不考虑 ippool 扩容的操作。
calico 支持直接修改 ippool 资源的 cidr，因此在未启用 ks ippool 的情况下可以直接扩容的 ippool cidr，修改时不要与已有的 ippool 或者已有的底层 ip 地址段冲突。
通过此方法扩容之后，ippool 的地址段增加，可用的 ipamblocks 增加，也不会对已有的业务造成影响。</p><blockquote><p>需要注意 calico ippool 没有做地址冲突检测等机制，因此在修改 calico 相关资源时需要明确自己的操作以及会带来的影响。</p></blockquote><p><strong>ks 界面中不支持扩容操作，可以进行删除后重建</strong>。
kubesphere 为了整个集群的网络安全考虑，不支持对 ippool 的修改；同时对于要删除的 ippool 必须保证其 ippool 中没有 ip 被占用，否则删除失败。
因此对于已经启用 kubesphere ippool 的用户，对于要扩容的 ippool，需要进行以下步骤：</p><ol><li>将 ippool 上的负载迁移到其他 ippool(<em>修改负载注解中使用的 ippool</em>)</li><li>删除要扩容的 ippool</li><li>新建较大范围的 ippool</li><li>将必要的负载迁移回该 ippool (<em>修改负载注解中使用的 ippool</em>)</li></ol><a href=#2-固定-ip><h3 id=2-固定-ip><span class=hanchor arialabel=Anchor># </span>2. 固定 ip</h3></a><p><a href=https://projectcalico.docs.tigera.io/reference/cni-plugin/configuration#requesting-a-specific-ip-address rel=noopener>Configure the Calico CNI plugins (tigera.io)</a>
<a href=https://projectcalico.docs.tigera.io/networking/use-specific-ip#reserving-ips-for-manual-assignments rel=noopener>Use a specific IP address with a pod (tigera.io)</a>
设置的 ip 地址需要在 calico ippool 范围中，否则 ipam 报错找不到相应的 ipamblock 无法分配 ip。
<strong>单个 pod 固定 ip</strong>
创建 pod 或者创建单个副本的 pod 时添加<code>cni.projectcalico.org/ipAddrs</code>注解：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl> annotations:
</span></span><span class=line><span class=cl>    &#34;cni.projectcalico.org/ipAddrs&#34;: &#34;[\&#34;192.168.0.1\&#34;]&#34;
</span></span></code></pre></td></tr></table></div></div><p>pod 对应的 annotation 的 ipv4 地址值只支持设置一个，当设置多个时会报错：
cannot have more than one IPv4 address for &ldquo;cni.projectcalico.org/ipAddrs&rdquo; annotation</p><p><strong>deploy 多副本 pod 固定 ip</strong>
先创建预期 cidr、有符合预期 ip 数量的 ippool
<img src=https://images.cherryfloris.eu.org/ryken/2022/12/upgit_20221222_1671700172.png width=auto alt=upgit_20221222_1671700172.png>
创建负载时添加<code>cni.projectcalico.org/ipv4pools</code>注解：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>annotations:
</span></span><span class=line><span class=cl>    &#34;cni.projectcalico.org/ipv4pools&#34;: &#34;[\&#34;ippool\&#34;]&#34;
</span></span></code></pre></td></tr></table></div></div><p>当副本数超过 ippool 大小时，pod 会创建不成功直到该 ippool 有空余 ip 。建议使用时 ippool 可用 ip 数量大于副本数，这样可以在进行扩容或者重启操作时避免出现因 ip 不够用，pod 等待释放 ip 而无法创建的过程</p><blockquote><p>如果同时存在 <code>cni.projectcalico.org/ipAddrs</code> 和 <code>cni.projectcalico.org/ipv4pools</code> 时，以最小粒度的 <code>cni.projectcalico.org/ipAddrs</code> 为主。</p></blockquote><a href=#3-ns-与-subnet-绑定><h4 id=3-ns-与-subnet-绑定><span class=hanchor arialabel=Anchor># </span>3. ns 与 subnet 绑定</h4></a><p><a href=https://projectcalico.docs.tigera.io/reference/cni-plugin/configuration#using-kubernetes-annotations rel=noopener>Configure the Calico CNI plugins (tigera.io)</a>
通过设置注解<code>cni.projectcalico.org/ipv4pools</code>来实现 ns 与 ippool 的绑定</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>annotations:
</span></span><span class=line><span class=cl>    &#34;cni.projectcalico.org/ipv4pools&#34;: &#34;[\&#34;ippool\&#34;]&#34;
</span></span></code></pre></td></tr></table></div></div><p>可以绑定多个 ippool，其使用顺序一般为：指定的第一个 ippool 用完之后，使用指定的第二个 ippool。对于 ip 的分配同样是使用 ippool - ipamblock 的方式分配。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>annotations:
</span></span><span class=line><span class=cl>    &#34;cni.projectcalico.org/ipv4pools&#34;: &#34;[\&#34;ippool\&#34;, \&#34;ippool-1\&#34;]&#34;
</span></span></code></pre></td></tr></table></div></div><hr><p>tor 建立 ibgp 情况下，可能学习不到路由 or blackhole 路由无法借用地址，或者借用了地址无法访问</p><hr><a href=#4-calico-限速><h4 id=4-calico-限速><span class=hanchor arialabel=Anchor># </span>4. calico 限速</h4></a><p><a href=/bandwidth-%E5%B8%A6%E5%AE%BD%E9%99%90%E5%88%B6 rel=noopener class=internal-link data-src=/bandwidth-%E5%B8%A6%E5%AE%BD%E9%99%90%E5%88%B6>bandwidth 带宽限制</a>
在 kubernetes 中有两种方式可以实现限流。</p><ol><li>配置 cni 插件 bandwidth</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;bandwidth&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;capabilities&#34;</span><span class=p>:</span> <span class=p>{</span><span class=nt>&#34;bandwidth&#34;</span><span class=p>:</span> <span class=kc>true</span><span class=p>},</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;ingressRate&#34;</span><span class=p>:</span> <span class=mi>10000000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;ingressBurst&#34;</span><span class=p>:</span> <span class=mi>10000000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;egressRate&#34;</span><span class=p>:</span> <span class=mi>10000000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;egressBurst&#34;</span><span class=p>:</span> <span class=mi>10000000</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>bandwidth 使用 tbf 实现流量整形。TBF (令牌桶过滤器) 是一个简单的队列规则，<strong>只允许以不超过事先设定的速率到来的数据包通过，但可能允许短暂突发流量超过设定值</strong>。TBF的实现在于一个缓冲器(桶)，桶被一些叫做”令牌”的虚拟数据以特定速率(token rate)填充着，如果有token 就能发数据。</p><p>cni 配置</p><ul><li>ingressRate：设定网卡收包速率 (bps)</li><li>ingressBurst：最多可以有多少个令牌能够即刻被使用(比特数)</li><li>egressRate：设定网卡发包速率 (bps)</li><li>egressBurst：最多可以有多少个令牌能够即刻被使用(比特数)，该参数同 tbf 的 burst 表示桶的大小。通常管理的带宽越大，需要的缓冲器就越大。在Intel体系上,10mbit/s的速率需要至少10k字节的缓冲区才能达到期望的速率。</li></ul><p>如果缓冲区太小，就会导致到达的令牌没有地方放(桶满了)，这会导致潜在的丢包。(最小缓冲区大小可以通过以下方式计算：速率/HZ = 10mbit / 100 ≈ 10kB)</p><blockquote><p>ingressRate 和 ingressBurst 必须同时设置，缺少一个则配置不生效。
egressRate 和 egressBurst 必须同时设置，缺少一个则配置不生效。
ingressBurst、egressBurst 在换算时会除以8，因此在填写实际值时要留意。</p><p>更多相关 tbf：
<a href=https://man7.org/linux/man-pages/man8/tbf.8.html rel=noopener>tc-tbf(8) - Linux manual page (man7.org)</a></p></blockquote><p><strong>calico 修改 cni 配置</strong>：
calico 默认的配置文件为configmap kube-system/calico-config。可以通过以下命令进行查看：<code>kubectl get cm -n kube-system calico-config -o yaml</code>。 calico 默认启用 bandwidth 来对 pod 入口和出口流量进行整形。因此可以根据上述 cni 配置来修改 calico-config，由此设置默认的带宽限制。</p><p>在 calico calico-config 配置中添加以上配置后，需要重启 calico-node daemonset，重启完成之后检查主机上 /etc/cni/net.d/10-calico.conflist 中是否也添加了对应的字段。</p><p>对于修改配置之前创建的 pod ，需要重启 pod 带宽限速才会生效。</p><ol start=2><li>设置 pod annotations</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kubernetes.io/ingress-bandwidth</span><span class=p>:</span><span class=w> </span><span class=l>300M</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kubernetes.io/egress-bandwidth</span><span class=p>:</span><span class=w> </span><span class=l>300M</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>该方法只能设置带宽，无法设置 Burst 值，对于 Burst 值，使用了较大的 256Mb。
<img src=https://images.cherryfloris.eu.org/ryken/2022/12/upgit_20221223_1671790862.png width=auto alt=upgit_20221223_1671790862.png></p><blockquote><p>经过测试发现，当 cni 配置带宽限制时，再次设置 pod 注解并不会覆盖掉 cni 的效果。</p></blockquote><a href=#5-calico-的-tunnel-地址会自动分配><h4 id=5-calico-的-tunnel-地址会自动分配><span class=hanchor arialabel=Anchor># </span>5. calico 的 tunnel 地址会自动分配</h4></a><p><strong>问题现象：</strong>
使用 ipip 模式安装集群，并切换为 bgp 模式之后(修改默认 default-ipv4-ippool 的 ipipMode: Never，并修改 calico-node daemonset 的环境变量 CALICO_IPV4POOL_IPIP: Never)，对于新建 ippool 的 ipipMode 为 Always 并且自动为 tunl0 分配 ip，在界面上删除 ippool 时提示 ip 被占用无法删除 ippool。
<strong>问题分析：</strong>
在界面上创建 ippool 时，由 ks-controller-manager 将 ks ippool 转换为 calico ippool，ks-controller-manager 在初始化时会获取 calico-node daemonset 的环境变量，并以此作为转换 calico ippool 的参数。ks-controller-manager 不会持续 watch calico-node daemonset， 因此在修改其环境变量后，ks-controller-manager 已经被初始化的参数不会自动修改。
<strong>问题解决：</strong>
修改为 bgp 模式并修改 calico-node daemonset 环境变量之后，重启一下 ks-controller-manager 即可；或者在使用 kk 安装集群时指定 ipipMode 为 Never。</p><a href=#6-ippool-在迁移业务时需要做的准备><h4 id=6-ippool-在迁移业务时需要做的准备><span class=hanchor arialabel=Anchor># </span>6. ippool 在迁移业务时需要做的准备</h4></a><p>在应用大规模更新时，大量容器删除并且大量容器创建，创建的容器需要很长时间才能就绪。这其中一个可能的原因，就是大量容器删除释放ip过于缓慢，导致新创建的容器无法及时获取ip，从而无法及时启动。(可用 ip 数量紧张)</p><p>kubernetes 容器的整个删除流程基本套路如下：</p><ol><li>发送删除请求到apiserver，标记容器的deletionTimestamp</li><li>kubelet watch到该事件，进入删除流程</li><li>pod执行killPod流程</li><li>kill app容器<ul><li>执行preStopHook</li><li>等待gracePeriod</li><li>停止app容器</li></ul></li><li>kill pause容器<ul><li>调用cni接口，停止容器网络</li><li>停止pause容器</li></ul></li><li>从apiserver中将pod的信息清除（真正删除掉存储在etcd的pod信息）</li></ol><p>可以看出，ip的释放其实是发生在调用cni接口的时候。因此，按照流程需要等待的时间是较为漫长的。</p><p>但是不同的更新策略，ip 数量紧张的情况也有所不同。</p><a href=#重建策略><h5 id=重建策略><span class=hanchor arialabel=Anchor># </span>重建策略</h5></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>strategy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Recreate</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>在该策略下更新，所有的 pod 会先处于 Terminating 状态，在所有的 Pod 都被删除之后才会开始创建新的 Pod。而 pod 在删除之后其占用的 ip 已经被释放，因此在迁移的过程中且在没有新的负载创建的情况下， ip 数量使用情况不会紧张，也不会产生 pod 等待释放 ip 后才能分配 ip 的情况。</p><a href=#滚动更新><h5 id=滚动更新><span class=hanchor arialabel=Anchor># </span>滚动更新</h5></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>strategy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollingUpdate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxSurge</span><span class=p>:</span><span class=w> </span><span class=m>25</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxUnavailable</span><span class=p>:</span><span class=w> </span><span class=m>25</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>RollingUpdate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>strategy</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>RollingUpdate</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>rollingUpdate</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxSurge</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>        </span><span class=c># 一次可以添加多少个Pod</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>maxUnavailable</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>  </span><span class=c># 滚动更新期间最大多少个Pod不可用</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>在应用更新时如果采用滚动更新，需要确保可用的Pod对象数量不低于某阈值以确保可以持续处理客户端的服务请求。此时对于新的 pod 创建的时机以及整个迁移的进度除了受是否有可用 ip 的影响外，同时也受更新策略设定的 <code>spec.strategy.rollingUpdate.maxSurge</code>和<code>spec.strategy.rollingUpdate.maxUnavailable</code>两个属性影响。</p><ul><li> maxSurge：和期望ready的副本数比，超过期望副本数最大比例（或最大值），<strong>这个值调的越大，副本更新速度越快</strong>。假设当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%。一旦旧 Pod 被杀死，新的 ReplicaSet 可以进一步扩容， 同时确保更新期间的任何时候运行中的 Pod 总数最多为所需 Pod 总数的 130%。</li><li> maxUnavailable：和期望ready的副本数比，不可用副本数最大比例（或最大值），<strong>这个值越小，越能保证服务稳定，更新越平滑</strong>；假设当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。 新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容， 确保在更新期间可用的 Pod 总数在任何时候都至少为所需的 Pod 个数的 70%。</li></ul><p>滚动更新时会出现一部分 pod 处于删除中，ip 没有被释放，新的一部分 pod 处于新建中，正在进行分配 ip；此时的 ip 占用数量大于期望的副本数量。可能出现 ip 数量紧张、新建的 pod 等待删除的 pod 释放 ip 的情况。因此使用滚动更新迁移应用时，最好保证 ippool 中可用 ip 数量 ≥ maxSurge(比例) * 现有负载 pod 数。</p><a href=#7-blocksize-取值><h4 id=7-blocksize-取值><span class=hanchor arialabel=Anchor># </span>7. blockSize 取值</h4></a><p>calico 使用 block 进行 ip 分配，blockSize 为 block cidr 的大小。取值范围为20~32，默认值为 26。如果创建的 ippool 的 cidr 大于 26，则 blockSize 需要指定，否则创建的 ippool 不可使用。
当 ippool cidr 设置为 24，blockSize 设置为 26 时，可以分为 4 个 block。如果此时节点数量为 2，当一个节点上 block 中 ip 占用满时，会继续申请一个 block 并绑定到该节点上；如果节点数量为4，刚好每个节点分配一个 block，当节点 block 中 ip 使用完之后，便会出现借用 ip 或者无法创建 pod 的情况（根据 ipamconfig 配置决定）；如果节点数量大于 4 时，block 数量不够用，所以必然会出现 ip 借用或者无法创建 pod 的情况。
因此首先保证 ippool 中的 ip 充足，其次尽量使 block 可以均匀的分配到节点上，避免 ip 借用导致路由数增多或 pod 无法创建的问题。综合考虑之后，计算出适当的 blockSize 值。</p><p>参考：</p><ul><li><a href=https://stackoverflow.com/questions/59729354/set-burst-for-bandwidth-limit-for-a-pod rel=noopener>kubernetes - Set burst for bandwidth limit for a pod - Stack Overflow</a></li><li><a href=https://www.cni.dev/plugins/current/meta/bandwidth/ rel=noopener>CNI</a></li><li><a href=https://man7.org/linux/man-pages/man8/tbf.8.html rel=noopener>tc-tbf(8) - Linux manual page (man7.org)</a></li><li><a href=https://blog.csdn.net/zhonglinzhang/article/details/98053900 rel=noopener>【kubernetes/k8s源码分析】CNI bandwidth源码分析_张忠琳的博客-CSDN博客_bandwidth cni</a></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>反向链接</h3><ul class=backlinks><li>未发现反向链接</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>内部链接关系图</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://www.ryken.cloud/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><hr><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script src=alittlejs/md5.min.js></script><div id=gitalk-container></div><script>var gitalk=new Gitalk({clientID:"d21f4afcd185c95004e0",clientSecret:"8f1f442abf93a76461783af21c544717da9fc7b3",repo:"obsidian-publish",owner:"renyunkang",admin:["renyunkang"],id:location.href,distractionFreeMode:!1});gitalk.render("gitalk-container")</script><center><script>var caution=!1,now,visits;function setCookie(e,t,n,s,o,a){var i=e+"="+escape(t)+(n?"; expires="+n.toGMTString():"")+(s?"; path="+s:"")+(o?"; domain="+o:"")+(a?"; secure":"");!caution||(e+"="+escape(t)).length<=4e3?document.cookie=i:confirm("Cookie exceeds 4KB and will be cut!")&&(document.cookie=i)}function getCookie(s){var e=s+"=",n,t=document.cookie.indexOf(e);return t==-1?null:(n=document.cookie.indexOf(";",t+e.length),n==-1&&(n=document.cookie.length),unescape(document.cookie.substring(t+e.length,n)))}function deleteCookie(e,t,n){getCookie(e)&&(document.cookie=e+"="+(t?"; path="+t:"")+(n?"; domain="+n:"")+"; expires=Thu, 01-Jan-70 00:00:01 GMT")}function fixDate(e){var n=new Date(0),t=n.getTime();t>0&&e.setTime(e.getTime()-t)}now=new Date,fixDate(now),now.setTime(now.getTime()+730*24*60*60*1e3),visits=getCookie("counter"),visits?visits=parseInt(visits)+1:visits=1,setCookie("counter",visits,now),document.write("<font size=2>👋访问量："+visits+"")</script></center></div></body></html>