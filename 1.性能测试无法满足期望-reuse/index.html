<!doctype html><html lang=en><head><script async defer data-website-id=08a754c4-6690-46d7-bb80-8ff93cfa232f src=https://umami.oldwinter.top/umami.js></script><meta charset=utf-8><meta name=description content="在高并发、短连接的场景下，kube-proxy ipvs存在rs删除失败或是延迟高的问题，社区也有不少Issue反馈，比如 kube-proxy ipvs conn_reuse_mode setting causes errors with high load from single client。文本对这些问题进行了梳理，试图介绍产生这些问题的内部原因。由于能力有限，其中涉及内核部分，只能浅尝辄止。
背景 端口重用 一切问题来源于端口重用。在TCP四次挥手中有个TIME_WAIT的状态，作为先发送FIN包的一端，在接收到对端发送的FIN包后进入TIME_WAIT，在经过2MSL后才会真正关闭连接。TIME_WAIT状态的存在，一来可以避免将之前连接的延迟报文，作为当前连接的报文处理；二是可以处理最后一个ACK丢失带来的问题。
 而在短连接、高并发的场景下，会出现大量的TIME-WAIT连接，导致资源无法及时释放。Linux中内核参数net.ipv4.tcp_tw_reuse提供了一种减少TIME-WAIT连接的方式，可以将TIME-WAIT连接的端口分配给新的TCP连接，来复用端口。
1 2 3 4 5  tcp_tw_reuse - BOOLEAN Allow to reuse TIME-WAIT sockets for new connections when it is safe from protocol viewpoint."><title>1.性能测试无法满足期望 - reuse</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://www.ryken.cloud//icon.png><link href=https://www.ryken.cloud/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://www.ryken.cloud/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://www.ryken.cloud/js/darkmode.66c3ed4615b59e04fa897375e67b0fe2.min.js></script>
<script src=https://www.ryken.cloud/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://www.ryken.cloud/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://www.ryken.cloud/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://www.ryken.cloud/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://www.ryken.cloud/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://www.ryken.cloud/",fetchData=Promise.all([fetch("https://www.ryken.cloud/indices/linkIndex.c83b530c16d6d7da0b36ea1cd7188b9b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://www.ryken.cloud/indices/contentIndex.73dda025fa3e36b0455e9f192a3570a0.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addCollapsibleCallouts(),initPopover("https://www.ryken.cloud",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://www.ryken.cloud",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},window.addEventListener("DOMContentLoaded",()=>{init(),render()})</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-11MD77L81V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-11MD77L81V",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=search placeholder=支持标题及全文搜索...><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://www.ryken.cloud/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://www.ryken.cloud/>Knowledge Garden</a></h1><div class=spacer></div><div id=search-icon><p>search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>1.性能测试无法满足期望 - reuse</h1><p class=meta>最近编辑于
Apr 15, 2024</p><ul class=tags></ul><aside class=mainTOC><details><summary>目录</summary><nav id=TableOfContents><ol><li><a href=#背景>背景</a><ol><li><a href=#端口重用>端口重用</a></li><li><a href=#ipvs如何处理端口重用>ipvs如何处理端口重用？</a></li><li><a href=#kube-proxy-ipvs模式下的优雅删除>kube-proxy ipvs模式下的优雅删除</a></li></ol></li><li><a href=#kube-proxy-ipvs模式下的问题>kube-proxy ipvs模式下的问题</a></li><li><a href=#如何解决>如何解决</a></li><li><a href=#即将到来>即将到来</a></li><li><a href=#补充>补充</a></li></ol></nav></details></aside><p>在高并发、短连接的场景下，kube-proxy ipvs存在rs删除失败或是延迟高的问题，社区也有不少Issue反馈，比如
<a href=https://github.com/kubernetes/kubernetes/issues/81775 rel=noopener>kube-proxy ipvs conn_reuse_mode setting causes errors with high load from single client</a>。文本对这些问题进行了梳理，试图介绍产生这些问题的内部原因。由于能力有限，其中涉及内核部分，只能浅尝辄止。</p><a href=#背景><h2 id=背景><span class=hanchor arialabel=Anchor># </span>背景</h2></a><a href=#端口重用><h3 id=端口重用><span class=hanchor arialabel=Anchor># </span>端口重用</h3></a><p>一切问题来源于端口重用。在TCP四次挥手中有个<code>TIME_WAIT</code>的状态，作为先发送<code>FIN</code>包的一端，在接收到对端发送的<code>FIN</code>包后进入<code>TIME_WAIT</code>，在经过<code>2MSL</code>后才会真正关闭连接。<code>TIME_WAIT</code>状态的存在，一来可以避免将之前连接的延迟报文，作为当前连接的报文处理；二是可以处理最后一个ACK丢失带来的问题。</p><p><img src=https://images.cherryfloris.eu.org/ryken/2023/09/8405aacf3269254edde1ba0a96d4a027.png width=auto alt=image.png></p><p>而在短连接、高并发的场景下，会出现大量的<code>TIME-WAIT</code>连接，导致资源无法及时释放。Linux中内核参数<code>net.ipv4.tcp_tw_reuse</code>提供了一种减少<code>TIME-WAIT</code>连接的方式，可以将<code>TIME-WAIT</code>连接的端口分配给新的TCP连接，来复用端口。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>tcp_tw_reuse - BOOLEAN
</span></span><span class=line><span class=cl>	Allow to reuse TIME-WAIT sockets for new connections when it is
</span></span><span class=line><span class=cl>	safe from protocol viewpoint. Default value is 0.
</span></span><span class=line><span class=cl>	It should not be changed without advice/request of technical
</span></span><span class=line><span class=cl>	experts.
</span></span></code></pre></td></tr></table></div></div><a href=#ipvs如何处理端口重用><h3 id=ipvs如何处理端口重用><span class=hanchor arialabel=Anchor># </span>ipvs如何处理端口重用？</h3></a><p>ipvs对端口的复用策略主要由内核参数<code>net.ipv4.vs.conn_reuse_mode</code>决定</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>conn_reuse_mode - INTEGER
</span></span><span class=line><span class=cl>	1 - default
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	Controls how ipvs will deal with connections that are detected
</span></span><span class=line><span class=cl>	port reuse. It is a bitmap, with the values being:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	0: disable any special handling on port reuse. The new
</span></span><span class=line><span class=cl>	connection will be delivered to the same real server that was
</span></span><span class=line><span class=cl>	servicing the previous connection. This will effectively
</span></span><span class=line><span class=cl>	disable expire_nodest_conn.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	bit 1: enable rescheduling of new connections when it is safe.
</span></span><span class=line><span class=cl>	That is, whenever expire_nodest_conn and for TCP sockets, when
</span></span><span class=line><span class=cl>	the connection is in TIME_WAIT state (which is only possible if
</span></span><span class=line><span class=cl>	you use NAT mode).
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	bit 2: it is bit 1 plus, for TCP connections, when connections
</span></span><span class=line><span class=cl>	are in FIN_WAIT state, as this is the last state seen by load
</span></span><span class=line><span class=cl>	balancer in Direct Routing mode. This bit helps on adding new
</span></span><span class=line><span class=cl>	real servers to a very busy cluster.
</span></span></code></pre></td></tr></table></div></div><p>当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，ipvs不会对新连接进行重新负载，而是复用之前的负载结果，将新连接转发到原来的rs上；当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，ipvs则会对新连接进行重新调度。</p><p>相关的，还有一个内核参数<code>net.ipv4.vs.expire_nodest_conn</code>，用于控制连接的rs不可用时的处理。在开启时，如果后端rs不可用，会立即结束掉该连接，使客户端重新发起新的连接请求；否则将数据包<strong>silently drop</strong>，也就是DROP掉数据包但不结束连接，等待客户端的重试。</p><p>另外，关于<strong>destination 不可用</strong>的判断，是在ipvs执行删除<code>vs</code>（在<code>__ip_vs_del_service()</code>中实现）或删除<code>rs</code>（在<code>ip_vs_del_dest()</code>中实现）时，会调用<code>__ip_vs_unlink_dest()</code>方法，将相应的destination置为不可用。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>expire_nodest_conn - BOOLEAN
</span></span><span class=line><span class=cl>        0 - disabled (default)
</span></span><span class=line><span class=cl>        not 0 - enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        The default value is 0, the load balancer will silently drop
</span></span><span class=line><span class=cl>        packets when its destination server is not available. It may
</span></span><span class=line><span class=cl>        be useful, when user-space monitoring program deletes the
</span></span><span class=line><span class=cl>        destination server (because of server overload or wrong
</span></span><span class=line><span class=cl>        detection) and add back the server later, and the connections
</span></span><span class=line><span class=cl>        to the server can continue.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        If this feature is enabled, the load balancer will expire the
</span></span><span class=line><span class=cl>        connection immediately when a packet arrives and its
</span></span><span class=line><span class=cl>        destination server is not available, then the client program
</span></span><span class=line><span class=cl>        will be notified that the connection is closed. This is
</span></span><span class=line><span class=cl>        equivalent to the feature some people requires to flush
</span></span><span class=line><span class=cl>        connections when its destination is not available.
</span></span></code></pre></td></tr></table></div></div><p>关于ipvs如何处理端口复用的连接，这块主要实现逻辑在<code>net/netfilter/ipvs/ip_vs_core.c</code>的<code>ip_vs_in()</code>方法中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>/*
</span></span><span class=line><span class=cl> * Check if the packet belongs to an existing connection entry
</span></span><span class=line><span class=cl> */
</span></span><span class=line><span class=cl>cp = pp-&gt;conn_in_get(ipvs, af, skb, &amp;iph);  //找是属于某个已有的connection
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);
</span></span><span class=line><span class=cl>//当conn_reuse_mode开启，同时出现端口复用（例如收到TCP的SYN包，并且也属于已有的connection），进行处理
</span></span><span class=line><span class=cl>if (conn_reuse_mode &amp;&amp; !iph.fragoffs &amp;&amp; is_new_conn(skb, &amp;iph) &amp;&amp; cp) { 
</span></span><span class=line><span class=cl>	bool uses_ct = false, resched = false;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	//如果开启了expire_nodest_conn、目标rs的weight为0
</span></span><span class=line><span class=cl>	if (unlikely(sysctl_expire_nodest_conn(ipvs)) &amp;&amp; cp-&gt;dest &amp;&amp;
</span></span><span class=line><span class=cl>	    unlikely(!atomic_read(&amp;cp-&gt;dest-&gt;weight))) {
</span></span><span class=line><span class=cl>		resched = true;
</span></span><span class=line><span class=cl>		//查询是否用到了conntrack
</span></span><span class=line><span class=cl>		uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
</span></span><span class=line><span class=cl>	} else if (is_new_conn_expected(cp, conn_reuse_mode)) {
</span></span><span class=line><span class=cl>	//连接是expected的情况，比如FTP
</span></span><span class=line><span class=cl>		uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
</span></span><span class=line><span class=cl>		if (!atomic_read(&amp;cp-&gt;n_control)) {
</span></span><span class=line><span class=cl>			resched = true;
</span></span><span class=line><span class=cl>		} else {
</span></span><span class=line><span class=cl>			/* Do not reschedule controlling connection
</span></span><span class=line><span class=cl>			 * that uses conntrack while it is still
</span></span><span class=line><span class=cl>			 * referenced by controlled connection(s).
</span></span><span class=line><span class=cl>			 */
</span></span><span class=line><span class=cl>			resched = !uses_ct;
</span></span><span class=line><span class=cl>		}
</span></span><span class=line><span class=cl>	}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	//如果expire_nodest_conn未开启，并且也非期望连接，实际上直接跳出了
</span></span><span class=line><span class=cl>	if (resched) {
</span></span><span class=line><span class=cl>		if (!atomic_read(&amp;cp-&gt;n_control))
</span></span><span class=line><span class=cl>			ip_vs_conn_expire_now(cp);
</span></span><span class=line><span class=cl>		__ip_vs_conn_put(cp);
</span></span><span class=line><span class=cl>		//当开启了net.ipv4.vs.conntrack，SYN数据包会直接丢弃，等待客户端重新发送SYN
</span></span><span class=line><span class=cl>		if (uses_ct)
</span></span><span class=line><span class=cl>			return NF_DROP;
</span></span><span class=line><span class=cl>		//未开启conntrack时，会进入下面ip_vs_try_to_schedule的流程
</span></span><span class=line><span class=cl>		cp = NULL;
</span></span><span class=line><span class=cl>	}
</span></span><span class=line><span class=cl>}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>if (unlikely(!cp)) {
</span></span><span class=line><span class=cl>	int v;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	if (!ip_vs_try_to_schedule(ipvs, af, skb, pd, &amp;v, &amp;cp, &amp;iph))
</span></span><span class=line><span class=cl>		return v;
</span></span><span class=line><span class=cl>}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>IP_VS_DBG_PKT(11, af, pp, skb, iph.off, &#34;Incoming packet&#34;);
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>/* Check the server status */
</span></span><span class=line><span class=cl>if (cp-&gt;dest &amp;&amp; !(cp-&gt;dest-&gt;flags &amp; IP_VS_DEST_F_AVAILABLE)) {
</span></span><span class=line><span class=cl>	/* the destination server is not available */
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	__u32 flags = cp-&gt;flags;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	/* when timer already started, silently drop the packet.*/
</span></span><span class=line><span class=cl>	if (timer_pending(&amp;cp-&gt;timer))
</span></span><span class=line><span class=cl>		__ip_vs_conn_put(cp);
</span></span><span class=line><span class=cl>	else
</span></span><span class=line><span class=cl>		ip_vs_conn_put(cp);
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	if (sysctl_expire_nodest_conn(ipvs) &amp;&amp;
</span></span><span class=line><span class=cl>	    !(flags &amp; IP_VS_CONN_F_ONE_PACKET)) {
</span></span><span class=line><span class=cl>		/* try to expire the connection immediately */
</span></span><span class=line><span class=cl>		ip_vs_conn_expire_now(cp);
</span></span><span class=line><span class=cl>	}
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	return NF_DROP;
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><a href=#kube-proxy-ipvs模式下的优雅删除><h3 id=kube-proxy-ipvs模式下的优雅删除><span class=hanchor arialabel=Anchor># </span>kube-proxy ipvs模式下的优雅删除</h3></a><p>Kubernetes提供了Pod优雅删除机制。当我们决定干掉一个Pod时，我们可以通过<code>PreStop Hook</code>来做一些服务下线前的处理，同时Kubernetes也有个<code>grace period</code>，超过这个时间但未完成删除的Pod会被强制删除。</p><p>而在Kubernetes 1.13之前，kube-proxy ipvs模式并不支持优雅删除，当Endpoint被删除时，kube-proxy会直接移除掉ipvs中对应的rs，这样会导致后续的数据包被丢掉。</p><p>在1.13版本后，Kubernetes添加了
<a href=https://github.com/kubernetes/kubernetes/pull/66012 rel=noopener>IPVS优雅删除</a>的逻辑，主要是两点：</p><ul><li>当Pod被删除时，kube-proxy会先将rs的<code>weight</code>置为0，以防止新连接的请求发送到此rs，由于不再直接删除rs，旧连接仍能与rs正常通信；</li><li>当rs的<code>ActiveConn</code>数量为0（后面版本已改为<code>ActiveConn+InactiveConn==0</code>)，即不再有连接转发到此rs时，此rs才会真正被移除。</li></ul><a href=#kube-proxy-ipvs模式下的问题><h2 id=kube-proxy-ipvs模式下的问题><span class=hanchor arialabel=Anchor># </span>kube-proxy ipvs模式下的问题</h2></a><p>看上去kube-proxy ipvs的删除是优雅了，但当优雅删除正巧碰到端口重用，那问题就来了。</p><p>首先，kube-proxy希望通过设置<code>weight</code>为0，来避免新连接转发到此rs。但当<code>net.ipv4.vs.conn_reuse_mode=0</code>时，对于端口复用的连接，ipvs不会主动进行新的调度（调用<code>ip_vs_try_to_schedule</code>方法）；同时，只是将<code>weight</code>置为0，也并不会触发由<code>expire_nodest_conn</code>控制的结束连接或DROP操作，就这样，新连接的数据包当做什么都没发生一样，发送给了正在删除的Pod。这样一来，只要不断的有端口复用的连接请求发来，rs就不会被kube-proxy删除，上面提到的优雅删除的两点均无法实现。</p><p>而当<code>net.ipv4.vs.conn_reuse_mode=1</code>时，根据<code>ip_vs_in()</code>的处理逻辑，当开启了<code>net.ipv4.vs.conntrack</code>时，会DROP掉第一个SYN包，导致SYN的重传，有1S延迟。而Kube-proxy在IPVS模式下，使用了iptables进行<code>MASQUERADE</code>，也正好开启了<code>net.ipv4.vs.conntrack</code>。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>conntrack - BOOLEAN
</span></span><span class=line><span class=cl>	0 - disabled (default)
</span></span><span class=line><span class=cl>	not 0 - enabled
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	If set, maintain connection tracking entries for
</span></span><span class=line><span class=cl>	connections handled by IPVS.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	This should be enabled if connections handled by IPVS are to be
</span></span><span class=line><span class=cl>	also handled by stateful firewall rules. That is, iptables rules
</span></span><span class=line><span class=cl>	that make use of connection tracking.  It is a performance
</span></span><span class=line><span class=cl>	optimisation to disable this setting otherwise.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	Connections handled by the IPVS FTP application module
</span></span><span class=line><span class=cl>	will have connection tracking entries regardless of this setting.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	Only available when IPVS is compiled with CONFIG_IP_VS_NFCT enabled.
</span></span></code></pre></td></tr></table></div></div><p>这样看来，目前的情况似乎是，如果你需要实现优雅删除中的“保持旧连接不变，调度新连接”能力，那就要付出1s的延迟代价；如果你要好的性能，那么就不能重新调度。</p><a href=#如何解决><h2 id=如何解决><span class=hanchor arialabel=Anchor># </span>如何解决</h2></a><p>从Kubernetes角度来说，Kube-proxy需要在保证性能的前提下，找到一种能让新连接重新调度的方式。但目前从内核代码中可以看到，需要将参数设置如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>net.ipv4.vs.conntrack=0
</span></span><span class=line><span class=cl>net.ipv4.vs.conn_reuse_mode=1
</span></span><span class=line><span class=cl>net.ipv4.vs.expire_nodest_conn=1
</span></span></code></pre></td></tr></table></div></div><p>但Kube-proxy ipvs模式目前无法摆脱iptables来完成k8s service的转发。此外，Kube-proxy只有在<code>ActiveConn+InactiveConn==0</code>时才会删除rs，除此之外，在新的Endpoint和<code>GracefulTerminationList</code>（保存了<code>weight</code>为0，但暂未删除的rs）中的rs冲突时，才会立即删除rs。这种逻辑似乎并不合理。目前Pod已有优雅删除的逻辑，而kube-proxy应基于Pod的优雅删除，在网络层面做好rs的优雅删除，因此在kubelet完全删除Pod后，Kube-proxy是否也应该考虑同时删除相应的rs？</p><p>另外，从内核角度来说，ipvs需要提供一种方式，能在端口复用、同时使用conntrack的场景下，可以对新连接直接重新调度。</p><a href=#即将到来><h2 id=即将到来><span class=hanchor arialabel=Anchor># </span>即将到来</h2></a><p>这个问题在社区讨论一段时间后，目前出现的几个相关的解决如下：<br><strong>内核两个Patch</strong></p><ul><li><a href=http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200701151719.4751-1-ja@ssi.bg/ rel=noopener>ipvs: allow connection reuse for unconfirmed conntrack</a><br>修改了<code>ip_vs_conn_uses_conntrack()</code>方法的逻辑，当使用<code>unconfirmed conntrack</code>时，返回false，这种修改针对了TIME_WAIT的conntrack。</li><li><a href=http://patchwork.ozlabs.org/project/netfilter-devel/patch/20200708161638.13584-1-kim.andrewsy@gmail.com/ rel=noopener>ipvs: queue delayed work to expire no destination connections if expire_nodest_conn=1</a><br>提前了<code>expire connection</code>的操作，在destination被删除后，便开始将<code>expire connection</code>操作入队列。而不是等到数据包真正发过来时，才做<code>expire connection</code>，以此来减少数据包的丢失。</li></ul><p><strong>Kubernetes</strong><br><a href=https://github.com/kubernetes/enhancements/pull/1607 rel=noopener>Graceful Termination for External Traffic Policy Local</a><br><a href=https://github.com/kubernetes/kubernetes/pull/92968 rel=noopener>Add Terminating Condition to EndpointSlice</a><br>正如前面所说的，Kube-proxy需要能够感知到Pod的优雅删除过程，来同步进行rs的删除。目前，已有一个相应的KEP在进行中，通过在<code>Endpoint.EndpointConditions</code>中添加<code>terminating</code>字段，来为kube-proxy提供感知方式。</p><a href=#补充><h2 id=补充><span class=hanchor arialabel=Anchor># </span>补充</h2></a><ul><li><a href=https://maao.cloud/2021/01/15/%E6%B7%B1%E5%85%A5kube-proxy-ipvs%E6%A8%A1%E5%BC%8F%E7%9A%84conn_reuse_mode%E9%97%AE%E9%A2%98/ rel=noopener>深入kube-proxy ipvs模式的conn_reuse_mode问题</a></li><li><a href=https://support.huaweicloud.com/cce_faq/cce_faq_00409.html rel=noopener>CCE集群IPVS转发模式下conn_reuse_mode问题说明</a></li><li><a href=https://imroc.cc/k8s/faq/ipvs-conn-reuse-mode/ rel=noopener>ipvs 连接复用引发的系列问题</a>
<img src=https://images.cherryfloris.eu.org/ryken/2023/09/9a2b711b9b6c9e73680234bc20b4f0e2.png width=auto alt=image.png></li></ul><p><img src=https://images.cherryfloris.eu.org/ryken/2023/09/2ab9e6153213f00d44b895000a708970.png width=auto alt=image.png></p><p><img src=https://images.cherryfloris.eu.org/ryken/2023/09/96e7baf38359b88946a3e25e4445c6b3.png width=auto alt=image.png></p><p><img src=https://images.cherryfloris.eu.org/ryken/2023/09/6f50802fd1e56703e0d013652a247a24.png width=auto alt=image.png></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>反向链接</h3><ul class=backlinks><li>未发现反向链接</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>内部链接关系图</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://www.ryken.cloud/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><hr><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script src=alittlejs/md5.min.js></script><div id=gitalk-container></div><script>var gitalk=new Gitalk({clientID:"d21f4afcd185c95004e0",clientSecret:"8f1f442abf93a76461783af21c544717da9fc7b3",repo:"obsidian-publish",owner:"renyunkang",admin:["renyunkang"],id:location.href,distractionFreeMode:!1});gitalk.render("gitalk-container")</script><center><script>var caution=!1,now,visits;function setCookie(e,t,n,s,o,a){var i=e+"="+escape(t)+(n?"; expires="+n.toGMTString():"")+(s?"; path="+s:"")+(o?"; domain="+o:"")+(a?"; secure":"");!caution||(e+"="+escape(t)).length<=4e3?document.cookie=i:confirm("Cookie exceeds 4KB and will be cut!")&&(document.cookie=i)}function getCookie(s){var e=s+"=",n,t=document.cookie.indexOf(e);return t==-1?null:(n=document.cookie.indexOf(";",t+e.length),n==-1&&(n=document.cookie.length),unescape(document.cookie.substring(t+e.length,n)))}function deleteCookie(e,t,n){getCookie(e)&&(document.cookie=e+"="+(t?"; path="+t:"")+(n?"; domain="+n:"")+"; expires=Thu, 01-Jan-70 00:00:01 GMT")}function fixDate(e){var n=new Date(0),t=n.getTime();t>0&&e.setTime(e.getTime()-t)}now=new Date,fixDate(now),now.setTime(now.getTime()+730*24*60*60*1e3),visits=getCookie("counter"),visits?visits=parseInt(visits)+1:visits=1,setCookie("counter",visits,now),document.write("<font size=2>👋访问量："+visits+"")</script></center></div></body></html>